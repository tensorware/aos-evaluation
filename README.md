# AOS-Evaluation
Airborne Optical Sectioning Evaluation based on:

- David C. Schedl, Indrajit Kurmi, and Oliver Bimber, Search and rescue with airborne optical sectioning, Nature Machine Intelligence 2, 783-790, https://doi.org/10.1038/s42256-020-00261-3 (2020)
  - [Nature (final version)](https://www.nature.com/articles/s42256-020-00261-3) | [(view only version)](https://rdcu.be/cbcf2)
  - [arXiv (pre-print)](https://arxiv.org/pdf/2009.08835.pdf)
  - [Data: ](https://doi.org/10.5281/zenodo.3894773) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3894773.svg)](https://doi.org/10.5281/zenodo.3894773)
  - [Video on YouTube](https://www.youtube.com/watch?v=kyKVQYG-j7U)

> We show that automated person detection under occlusion conditions can be significantly improved by combining multi-perspective images before classification. Here, we employed image integration by Airborne Optical Sectioning (AOS) - a synthetic aperture imaging technique that uses camera drones to capture unstructured thermal light fields - to achieve this with a precision/recall of 96/93%. Finding lost or injured people in dense forests is not generally feasible with thermal recordings, but becomes practical with use of AOS integral images. Our findings lay the foundation for effective future search and rescue technologies that can be applied in combination with autonomous or manned aircraft. They can also be beneficial for other fields that currently suffer from inaccurate classification of partially occluded people, animals, or objects.

### Publications [![github](https://img.shields.io/badge/github-gray?logo=github&logoColor=white)](#Publications)

Additional publications and software modules for AOS based search and rescue can be found on the author's [main repository](https://github.com/JKU-ICG/AOS).

## Evaluation [![python](https://img.shields.io/badge/python-gray?logo=python&logoColor=white)](#Evaluation)
These evaluation scripts utilizes data generated by the [simulation](https://aos.tensorware.app). The source code for the simulation can be found [here](https://github.com/tensorware/aos-simulation).

### Process
The `process.ipynb` notebook extracts the .zip archive obtained from the simulation. This includes parsing of the .json config files and loading of .png image data. The image data is then processed and visualized, but the results are not stored for further analysis.

The `process.py` script does more or less the same as the notebook, but allows batch processing and an export of the resulting data. The preprocessed data is exported to a _results_ folder, which can then be further analyzed by the `visualize.ipynb` notebook.

### Visualize
The `visualize.ipynb` notebook loads the preprocessed data from the  _results_ folder. This includes parsing of the .json parameters file and loading of aggregated .npz numpy data. Those results can then be visualized by using various types of plots.

## License [![license](https://img.shields.io/badge/license-MIT-green)](#License)

[MIT](/LICENSE)
